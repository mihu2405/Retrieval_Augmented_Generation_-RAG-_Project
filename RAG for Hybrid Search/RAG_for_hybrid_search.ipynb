{"cells":[{"cell_type":"markdown","id":"61ea1c38-24ac-46ee-b8f5-fdeb65511131","metadata":{"id":"61ea1c38-24ac-46ee-b8f5-fdeb65511131"},"source":["# RAG Chat Bot for Hybrid Search\n","\n"]},{"cell_type":"markdown","id":"67f57977-9583-4772-86d9-e28e83dbe147","metadata":{"id":"67f57977-9583-4772-86d9-e28e83dbe147"},"source":["Quick notes:\n","- You will need an OpenAI API Key\n","- You will need a Pinecone API Key\n","\n"]},{"cell_type":"code","execution_count":null,"id":"bc3b8d11-6a04-4e7c-ace2-2bc28ab64b7f","metadata":{"id":"bc3b8d11-6a04-4e7c-ace2-2bc28ab64b7f"},"outputs":[],"source":["# Install libraries\n","\n","\n","!pip3 install -qU \\\n","  pinecone-client \\\n","  pinecone-text==0.5.4 \\\n","  unstructured==0.10.24 \\\n","  sentence-transformers==2.2.2 \\\n","  langchain==0.0.327 \\\n","  openai==0.28.1 \\\n","  pdfminer.six \\\n","  pdf2image==1.16.3 \\\n","  python-dotenv==1.0.0 \\\n","  pytesseract==0.3.10 \\\n","  pinecone-notebooks==0.1.1\\\n","  unstructured_pytesseract==0.3.12"]},{"cell_type":"code","execution_count":null,"id":"5g-la06jXtsn","metadata":{"id":"5g-la06jXtsn"},"outputs":[],"source":["!apt-get install poppler-utils"]},{"cell_type":"code","execution_count":null,"id":"6hrtPeeIa-I7","metadata":{"id":"6hrtPeeIa-I7"},"outputs":[],"source":["!sudo apt install tesseract-ocr\n","!sudo apt install libtesseract-dev"]},{"cell_type":"markdown","id":"6af345ef-6321-4a30-b2e3-9e84cbd4d5b2","metadata":{"id":"6af345ef-6321-4a30-b2e3-9e84cbd4d5b2"},"source":["Imports"]},{"cell_type":"code","execution_count":null,"id":"0520a370-63a3-4f16-8cf1-acfc8cc7f914","metadata":{"id":"0520a370-63a3-4f16-8cf1-acfc8cc7f914"},"outputs":[],"source":["from pinecone import Pinecone\n","import os\n","import re\n","from uuid import uuid4\n","from typing import IO, Any, Dict, List, Tuple\n","from copy import deepcopy\n","import requests\n","\n","from unstructured.partition.pdf import partition_pdf\n","from unstructured.documents.elements import Text\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.docstore.document import Document\n","from sentence_transformers import SentenceTransformer\n","from pinecone import Pinecone\n","import openai\n","from pinecone import QueryResponse\n","from pinecone_text.sparse import BM25Encoder"]},{"cell_type":"markdown","id":"c5858c70-9d3f-437d-a289-066ae69a64e2","metadata":{"id":"c5858c70-9d3f-437d-a289-066ae69a64e2"},"source":["Set environment variables for your Pinecone and OpenAI API keys:\n"]},{"cell_type":"code","execution_count":null,"id":"8fb61e9a-c191-489a-90e2-48df47e0c77b","metadata":{"id":"8fb61e9a-c191-489a-90e2-48df47e0c77b"},"outputs":[],"source":["import os\n","\n","# initialize connection to pinecone (orget API key at app.pinecone.io)\n","if not os.environ.get(\"PINECONE_API_KEY\"):\n","    from pinecone_notebooks.colab import Authenticate\n","    Authenticate()"]},{"cell_type":"code","execution_count":null,"id":"d62f04ff-5339-4097-b8e4-98a29450e879","metadata":{"id":"d62f04ff-5339-4097-b8e4-98a29450e879"},"outputs":[],"source":["\n","from google.colab import userdata\n","# available at platform.openai.com/api-keys\n","os.environ['OPENAI_API_KEY'] = userdata.get('openai_api')"]},{"cell_type":"code","execution_count":null,"id":"e0d6d9c5-2cf2-4796-8141-564e1b94994b","metadata":{"id":"e0d6d9c5-2cf2-4796-8141-564e1b94994b"},"outputs":[],"source":["pinecone_api_key = os.getenv('PINECONE_API_KEY')\n","openai_api_key = os.getenv('OPENAI_API_KEY')\n"]},{"cell_type":"markdown","id":"acfaeac9-d63a-4d5a-b316-f714f032c421","metadata":{"id":"acfaeac9-d63a-4d5a-b316-f714f032c421"},"source":["# Download some articles we're interested in learning more about.\n","\n","Remember, hybrid search is best for knowledge that contains a lot of unique keywords that you'd like to search for, along with concepts you'd like clarity on, etc. Data that works best for this type of thing include medical data, most types of research data, data with lots of entities in it, etc.\n","\n","We'll be using Arxiv.org articles about different vector search algorithms for this demo. They've got lots of jargon and concepts that'll work great for hybrid search!"]},{"cell_type":"code","execution_count":null,"id":"9cc05aea-08ac-493a-8bfe-6b0a90dbfb8f","metadata":{"id":"9cc05aea-08ac-493a-8bfe-6b0a90dbfb8f"},"outputs":[],"source":["import requests\n","import os\n","\n","def get_pdf(base_url: str, filename: str):\n","    \"\"\"\n","    Download and write a PDF file from a github repository.\n","\n","    :param url: URL of Github repository containing the file you want to download & write locally.\n","    \"\"\"\n","    res = requests.get(base_url+filename)\n","    # Check if the request was successful (HTTP status code 200)\n","    if res.status_code == 200:\n","      with open(filename, 'wb') as f:\n","          f.write(res.content)\n","          print(f\"PDF downloaded and saved as {filename}\")\n","    else:\n","      print(f\"Failed to download the PDF. HTTP status code: {res.status_code}\")"]},{"cell_type":"code","execution_count":null,"id":"c5f32da6-5ae9-47ac-a3d3-06bcdef1dfb7","metadata":{"id":"c5f32da6-5ae9-47ac-a3d3-06bcdef1dfb7"},"outputs":[],"source":["# Download our files to the /content/ dir in Colab\n","\n","github_dir = \"https://github.com/pinecone-io/examples/raw/master/learn/generation/rag-for-hybrid/\"\n","filenames = [\"freshdiskann_paper.pdf\", \"hnsw_paper.pdf\", \"ivfpq_paper.pdf\"]\n","\n","for f in filenames:\n","  get_pdf(github_dir, f)\n"]},{"cell_type":"code","execution_count":null,"id":"tpgyK79h8ta0","metadata":{"id":"tpgyK79h8ta0"},"outputs":[],"source":["# Read in our file paths\n","\n","freshdisk = os.path.join(\"/content/\", filenames[0])\n","hnsw = os.path.join(\"/content/\", filenames[1])\n","ivfpq = os.path.join(\"/content/\", filenames[2])\n"]},{"cell_type":"markdown","id":"c8e91a7b-2287-4200-b268-da25dec4f8c9","metadata":{"id":"c8e91a7b-2287-4200-b268-da25dec4f8c9"},"source":["# Partitioning & Cleaning our PDFs\n","\n","This step is optional. Partitioning simply uses ML to break a document up into pages, paragraphs, the title, etc. It's a nice-to-have that allows you to exclude certain elements you might not want to index, such as an article's bibliography (although we'll keep that since it could be useful information).\n","\n","If you want to skip this step, you can just read the PDFs into text or json, etc. and make your chunks straight from that object(s).\n","\n","Note: this notebook assumes you have partitioned your PDF. If you want to run this notebook from start to finish as-is, you'll need to run this step."]},{"cell_type":"code","execution_count":null,"id":"2fd90300-2c17-4b6d-bd12-624453c82fcc","metadata":{"id":"2fd90300-2c17-4b6d-bd12-624453c82fcc"},"outputs":[],"source":["# Let's partition all of our PDFs and store their partitions in a dictionary for easy retrieval & inspection later\n","\n","# Note: This takes a few mins to run (~12 mins; will be faster if running locally (~3 mins))\n","\n","partitioned_files = {\n","    \"freshdisk\": partition_pdf(freshdisk, url=None, strategy = 'ocr_only'),\n","    \"hnsw\": partition_pdf(hnsw, url=None, strategy = 'ocr_only'),\n","    \"ivfpq\": partition_pdf(ivfpq, url=None, strategy = 'ocr_only'),\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"b1cb1014-b774-42ba-8eb1-598762f3b434","metadata":{"id":"b1cb1014-b774-42ba-8eb1-598762f3b434"},"outputs":[],"source":["# Let's make an archived copy of partitioned_files dict so if we mess it up while cleaning, we don't have to re-ocr our PDFs:\n","\n","partitioned_files_copy = deepcopy(partitioned_files)"]},{"cell_type":"code","execution_count":null,"id":"ea7c9628-de78-45b1-946b-50e525e42f23","metadata":{"id":"ea7c9628-de78-45b1-946b-50e525e42f23"},"outputs":[],"source":["partitioned_files.get('freshdisk')"]},{"cell_type":"markdown","id":"1173112f-8449-4449-b520-2a19740fd940","metadata":{"id":"1173112f-8449-4449-b520-2a19740fd940"},"source":["You can see in the preview above that each of our PDFs now has elements classifying different parts of the text, such as `Text`, `Title`, and `EmailAddress`.\n","\n","Data cleaning matters a lot when it comes to hybrid search, because for the keyword-search part we care about each individual token (word).\n","\n","Let's filter out all of the email addresses to start with, since we don't need those for any reason."]},{"cell_type":"code","execution_count":null,"id":"a0236621-8c64-453a-bb8b-44b7019c6671","metadata":{"id":"a0236621-8c64-453a-bb8b-44b7019c6671","scrolled":true},"outputs":[],"source":["def remove_unwanted_categories(elements: Dict[str, List[Text]], unwanted_cat: str) -> None:\n","    \"\"\"\n","    Remove partitions containing an unwanted category.\n","\n","    :parameter elements: Partitioned pieces of our documents.\n","    :parameter unwanted_cat: The name of the category we'd like filtered out.\n","    \"\"\"\n","    for key, value in elements.items():\n","        elements[key] = [i for i in value if not i.category == unwanted_cat]\n"]},{"cell_type":"code","execution_count":null,"id":"00045ae3-4821-4200-a2fa-a3be94b216c1","metadata":{"id":"00045ae3-4821-4200-a2fa-a3be94b216c1"},"outputs":[],"source":["# Remove unwanted EmailAddress category from dictionary of partitioned PDFs\n","\n","remove_unwanted_categories(partitioned_files, 'EmailAddress')"]},{"cell_type":"markdown","id":"f9e35647-586a-4887-975e-bb86d091b7cd","metadata":{"id":"f9e35647-586a-4887-975e-bb86d091b7cd"},"source":["No more `EmailAddress` elements!:"]},{"cell_type":"code","execution_count":null,"id":"644d52c1-a0dd-4209-87c7-86f022630111","metadata":{"id":"644d52c1-a0dd-4209-87c7-86f022630111"},"outputs":[],"source":["partitioned_files.get('freshdisk')\n"]},{"cell_type":"markdown","id":"bb46a39a-8dcd-46cd-bbfb-f755d1052754","metadata":{"id":"bb46a39a-8dcd-46cd-bbfb-f755d1052754"},"source":["To actually see what our elements are, we can call the `.text` attribute of each object:"]},{"cell_type":"code","execution_count":null,"id":"dbabf1e0-656e-42e2-af4a-f6a3708e1cf2","metadata":{"id":"dbabf1e0-656e-42e2-af4a-f6a3708e1cf2"},"outputs":[],"source":["# Text preview of what's actually in one of our dictionary items:\n","\n","[i.text for i in partitioned_files.get('freshdisk')]"]},{"cell_type":"markdown","id":"bfade5a5-baa1-4479-8deb-8968cb8376ba","metadata":{"id":"bfade5a5-baa1-4479-8deb-8968cb8376ba"},"source":["You can see there are weird things like blank spaces, single letters, etc. as their own partitions. We don't want these either, so let's get rid of them.\n","\n","You can also see where some page breaks were that spanned single words -- these are identifiable by a word ending with a `- `. For these, we want to get rid of the `- ` and squish the word back together, so it makes sense.\n","\n","(You can also see that not all of the email addresses were caught by Unstructured's ML. It's too cumbersome to go through each doc and weed those out by hand, so we'll just have to leave them for now)"]},{"cell_type":"code","execution_count":null,"id":"22413a4e-1b61-41c6-83f3-1c7e3cc1ff54","metadata":{"id":"22413a4e-1b61-41c6-83f3-1c7e3cc1ff54"},"outputs":[],"source":["# Remove empty spaces & single-letter/-digit partitions:\n","\n","def remove_space_and_single_partitions(elements: Dict[str, List[Text]]) -> None:\n","    \"\"\"\n","    Remove empty partitions & partitions with lengths of 1.\n","\n","    :parameter elements: Partitioned pieces of our documents.\n","    \"\"\"\n","    for key, value in elements.items():\n","        elements[key] = [i for i in value if len(i.text.strip()) > 1 ]"]},{"cell_type":"code","execution_count":null,"id":"33e8aa65-d26e-4fc7-8f34-0ec2bc72da00","metadata":{"id":"33e8aa65-d26e-4fc7-8f34-0ec2bc72da00"},"outputs":[],"source":["remove_space_and_single_partitions(partitioned_files)"]},{"cell_type":"markdown","id":"562a2d57-7f94-4ff9-8ef0-96e5fa1ed7d2","metadata":{"id":"562a2d57-7f94-4ff9-8ef0-96e5fa1ed7d2"},"source":["No more single-character partitions or partitions with only whitespace, perfect!"]},{"cell_type":"code","execution_count":null,"id":"b6a7a897-8803-40eb-baaa-9dce221c03b3","metadata":{"id":"b6a7a897-8803-40eb-baaa-9dce221c03b3"},"outputs":[],"source":["[i.text for i in partitioned_files.get('freshdisk')]"]},{"cell_type":"markdown","id":"5c439c74-8a6b-4962-af7e-7e8d450e5b54","metadata":{"id":"5c439c74-8a6b-4962-af7e-7e8d450e5b54"},"source":["Let's now get rid of those strange words that have been split across page breaks (e.g. `funda- mental`):"]},{"cell_type":"code","execution_count":null,"id":"7e62b963-64eb-4b03-a50c-7ae5d98e7197","metadata":{"id":"7e62b963-64eb-4b03-a50c-7ae5d98e7197"},"outputs":[],"source":["# Note: this function transforms our elemenets into their text representations\n","\n","def rejoin_split_words(elements: Dict[str, List[Text]]) -> None:\n","    \"\"\"\n","    Rejoing words that are split over pagebreaks.\n","\n","    :parameter elements: Partitioned pieces of our documents.\n","    \"\"\"\n","    for key, value in elements.items():\n","        elements[key] = [i.text.replace('- ', '') for i in value if '- ' in i.text]\n","\n"]},{"cell_type":"code","execution_count":null,"id":"ee954160-75f7-44b4-a5c1-5bda0037fcce","metadata":{"id":"ee954160-75f7-44b4-a5c1-5bda0037fcce"},"outputs":[],"source":["rejoin_split_words(partitioned_files)"]},{"cell_type":"code","execution_count":null,"id":"1eaf8942-ed2b-4053-9a51-55a678ed8909","metadata":{"id":"1eaf8942-ed2b-4053-9a51-55a678ed8909"},"outputs":[],"source":["partitioned_files.get('freshdisk')"]},{"cell_type":"markdown","id":"5325d394-1266-4292-bf46-1d2d41a798ad","metadata":{"id":"5325d394-1266-4292-bf46-1d2d41a798ad"},"source":["You can see now that we've sewn those split words back together:"]},{"cell_type":"markdown","id":"551b2690-d91f-44a6-9969-fe2cc2142cc5","metadata":{"id":"551b2690-d91f-44a6-9969-fe2cc2142cc5"},"source":["The last cleaning step we'll want to take is removing the inline citations, e.g. `[6, 9, 11, 16, 32, 35, 38, 43, 59]` and `[12]`."]},{"cell_type":"code","execution_count":null,"id":"6383895a-72ec-4923-aa00-f2848822d35e","metadata":{"id":"6383895a-72ec-4923-aa00-f2848822d35e"},"outputs":[],"source":["def remove_inline_citation_numbers(elements: Dict[str, List[Text]]) -> None:\n","    \"\"\"\n","    Remove inline citation numbers from partitions.\n","\n","    :parameter elements: Partitioned pieces of our documents.\n","    \"\"\"\n","    for key, value in elements.items():\n","        pattern = re.compile(r'\\[\\s*(\\d+\\s*,\\s*)*\\d+\\s*\\]')\n","        elements[key] = [pattern.sub('', i) for i in value]\n","\n"]},{"cell_type":"code","execution_count":null,"id":"c59afe18-b664-4a0a-8103-91049ecb9bca","metadata":{"id":"c59afe18-b664-4a0a-8103-91049ecb9bca"},"outputs":[],"source":["remove_inline_citation_numbers(partitioned_files)"]},{"cell_type":"markdown","id":"17d99d35-605f-4dfc-af6f-47488aa2c7ab","metadata":{"id":"17d99d35-605f-4dfc-af6f-47488aa2c7ab"},"source":["We've still got some weird numbers in there, but it's pretty good!"]},{"cell_type":"code","execution_count":null,"id":"8f6cb1c5-ea66-4be9-9246-ae8a39e4ab28","metadata":{"id":"8f6cb1c5-ea66-4be9-9246-ae8a39e4ab28"},"outputs":[],"source":["partitioned_files.get('freshdisk')"]},{"cell_type":"markdown","id":"a653859f-4a0e-44b8-b49d-dc904cb141c9","metadata":{"id":"a653859f-4a0e-44b8-b49d-dc904cb141c9"},"source":["Now that we've cleaned our data, we can zip all the partitions (per PDF) back together so we're starting our chunking from a single, coherent text object."]},{"cell_type":"code","execution_count":null,"id":"7a8190b3-7d9d-45cd-8463-4e47729f9016","metadata":{"id":"7a8190b3-7d9d-45cd-8463-4e47729f9016"},"outputs":[],"source":["# Sew our partitions back together, per PDF:\n","\n","def stitch_partitions_back_together(elements: Dict[str, List[Text]]) -> None:\n","    \"\"\"\n","    Stitch partitions back into single string object.\n","\n","    :parameter elements:  Partitioned pieces of our documents.\n","    \"\"\"\n","    for key, value in elements.items():\n","        elements[key] = ' '.join(value)"]},{"cell_type":"code","execution_count":null,"id":"c7051c57-483e-4262-92a1-84dfa00c3f6e","metadata":{"id":"c7051c57-483e-4262-92a1-84dfa00c3f6e"},"outputs":[],"source":["stitch_partitions_back_together(partitioned_files)"]},{"cell_type":"markdown","id":"afc64d40-6205-4371-9b0b-a503b8fb45ef","metadata":{"id":"afc64d40-6205-4371-9b0b-a503b8fb45ef"},"source":["Good to go! All of our PDFs are now cleaned and single globs of text data"]},{"cell_type":"code","execution_count":null,"id":"0a985d1a-caa0-4e91-a510-0b8b440f1a22","metadata":{"id":"0a985d1a-caa0-4e91-a510-0b8b440f1a22","scrolled":true},"outputs":[],"source":["partitioned_files"]},{"cell_type":"code","execution_count":null,"id":"63780cff-5390-41a4-b53d-81772bcb800f","metadata":{"id":"63780cff-5390-41a4-b53d-81772bcb800f"},"outputs":[],"source":["# Let's save our cleaned files to a new variable that makes more sense w/the current state\n","\n","cleaned_files = partitioned_files"]},{"cell_type":"markdown","id":"36bc4e5e-aa4d-42bc-9bb3-d47bb46a1a1e","metadata":{"id":"36bc4e5e-aa4d-42bc-9bb3-d47bb46a1a1e"},"source":["# Chunking our PDF content\n","\n","Chunking is integral to achieving great relevance with vector search, whether that's sparse vector search, dense vector search, or hybrid vector search.\n","\n","[chunking strategy ](https://www.pinecone.io/learn/chunking-strategies/):\n","\n","> The main reason for chunking is to ensure we’re embedding a piece of content with as little noise as possible that is still semantically relevant . . . For example, in semantic search, we index a corpus of documents, with each document containing valuable information on a specific topic. By applying an effective chunking strategy, we can ensure our search results accurately capture the essence of the user’s query. If our chunks are too small or too large, it may lead to imprecise search results or missed opportunities to surface relevant content. As a rule of thumb, if the chunk of text makes sense without the surrounding context to a human, it will make sense to the language model as well. Therefore, finding the optimal chunk size for the documents in the corpus is crucial to ensuring that the search results are accurate and relevant.\n","\n","We need to chunk our PDFs' (text) data into sizable chunks that are semantically coherent and dense with contextual information.\n","\n","We'll use LangChain's `RecusiveCharacterTextSplitter` since it's a super easy utility that makes chunking quick and customizable. You should experiment with different chunk sizes and overlap values to see how the resulting chunks differ. You want each chunk to make a reasonable amount of sense as a stand-alone data object. After some experimentation on our end, we will choose a `chunk_size` of `512` and a `chunk_overlap` of `35` (characters)."]},{"cell_type":"code","execution_count":null,"id":"d1b8f24e-0052-41e7-9a23-8db8a6e829bb","metadata":{"id":"d1b8f24e-0052-41e7-9a23-8db8a6e829bb"},"outputs":[],"source":["def generate_chunks(doc: str, chunk_size: int = 512, chunk_overlap: int = 35) -> List[Document]:\n","    \"\"\"\n","    Generate chunks of a certain size and token overlap.\n","\n","    :param doc: Document we want to turn into chunks.\n","    :param chunk_size: Desired size of our chunks, in tokens (words).\n","    :param chunk_overlap: Desired # of tokens (words) that will overlap across chunks.\n","\n","    :return: Chunks representations of the given document.\n","    \"\"\"\n","    splitter = RecursiveCharacterTextSplitter(\n","        chunk_size = chunk_size,\n","        chunk_overlap = chunk_overlap\n","    )\n","\n","    return splitter.create_documents([doc])\n","\n"]},{"cell_type":"code","execution_count":null,"id":"667392aa-ba7b-4f3a-8fb9-2b63e6066b95","metadata":{"id":"667392aa-ba7b-4f3a-8fb9-2b63e6066b95"},"outputs":[],"source":["def chunk_documents(docs: Dict[str, List[Text]],  chunk_size: int = 512, chunk_overlap: int = 35) -> None:\n","    \"\"\"\n","    Iterate over documents and chunk each one.\n","\n","    :parameter docs: The documents we want to chunk.\n","    :param chunk_size: Desired size of our chunks, in tokens (words).\n","    :param chunk_overlap: Desired # of tokens (words) that will overlap across chunks.\n","    \"\"\"\n","    for key, value in docs.items():\n","        chunks = generate_chunks(value)\n","        docs[key] = [c.page_content for c in chunks]  # Grab the text representation of the chunks via the `page_content` attribute\n"]},{"cell_type":"code","execution_count":null,"id":"1a912cf2-4e14-47c5-a074-bcfd983056d9","metadata":{"id":"1a912cf2-4e14-47c5-a074-bcfd983056d9"},"outputs":[],"source":["chunk_documents(cleaned_files)"]},{"cell_type":"code","execution_count":null,"id":"94320c93-1993-40ee-850e-687a71fe3da4","metadata":{"id":"94320c93-1993-40ee-850e-687a71fe3da4"},"outputs":[],"source":["chunked_files = cleaned_files"]},{"cell_type":"markdown","id":"5c646c92-3c4e-401a-a6e7-0481501ada96","metadata":{"id":"5c646c92-3c4e-401a-a6e7-0481501ada96"},"source":["Check out our chunks!"]},{"cell_type":"code","execution_count":null,"id":"8ceb3744-e564-472d-a800-e50ee355681a","metadata":{"id":"8ceb3744-e564-472d-a800-e50ee355681a"},"outputs":[],"source":["chunked_files\n"]},{"cell_type":"markdown","id":"3e710a40-19b0-4957-89a6-30eaba4085d0","metadata":{"id":"3e710a40-19b0-4957-89a6-30eaba4085d0"},"source":["# Create Dense Embeddings of our Chunks\n","\n","Hybrid search needs both dense embeddings and sparse embeddings of the same content in order to work. Let's start with dense embeddings.\n","\n","We'll use the `'all-MiniLM-L12-v2'` [model](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2) hosted by HuggingFace to create our dense embeddings. It's currently high on their [MTEB (Massive Text Embedding Benchmark) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) (Reranking section), so it's a pretty safe bet. This will output dense vectors of 384 dimensions.\n","\n","Note: if you're playing around with this notebook, make sure to save your chunks and embeddings (both sparse and dense) in `pkl` [files](https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object), so that you don't have to wait for the embeddings to generate again if you want to rerun any steps in this notebook."]},{"cell_type":"markdown","id":"079ab428-d7d5-4aab-8ed3-e05408a5bfe3","metadata":{"id":"079ab428-d7d5-4aab-8ed3-e05408a5bfe3"},"source":["We'll have to create a dense embedding of each of our PDFs' chunks:"]},{"cell_type":"code","execution_count":null,"id":"3984fa37-32e3-43f3-9b97-b0013497a88e","metadata":{"id":"3984fa37-32e3-43f3-9b97-b0013497a88e"},"outputs":[],"source":["def produce_embeddings(chunks: List[str]) -> List[str]:\n","    \"\"\"\n","    Produce dense embeddings for each chunk.\n","\n","    :param chunks: The chunks we want to create dense embeddings of.\n","\n","    :return: Dense embeddings produced by our SentenceTransformer model `all-MiniLM-L12-v2`.\n","    \"\"\"\n","    model = SentenceTransformer('all-MiniLM-L12-v2')\n","    embeddings = []\n","    for c in chunks:\n","        embedding = model.encode(c)\n","        embeddings.append(embedding)\n","    return embeddings\n"]},{"cell_type":"code","execution_count":null,"id":"836ae697-df61-435f-b6da-093b0151e7b1","metadata":{"id":"836ae697-df61-435f-b6da-093b0151e7b1"},"outputs":[],"source":["freshdisk_dembeddings = produce_embeddings(chunked_files.get('freshdisk'))  # these take ~30s min to run"]},{"cell_type":"code","execution_count":null,"id":"7f8bb67f-2b95-48f6-bbfa-ed3bd40a1574","metadata":{"id":"7f8bb67f-2b95-48f6-bbfa-ed3bd40a1574"},"outputs":[],"source":["hnsw_dembeddings = produce_embeddings(chunked_files.get('hnsw'))"]},{"cell_type":"code","execution_count":null,"id":"fc992bc7-6c15-43e9-a88f-1f70412ea90c","metadata":{"id":"fc992bc7-6c15-43e9-a88f-1f70412ea90c"},"outputs":[],"source":["ivfpq_dembeddings = produce_embeddings(chunked_files.get('ivfpq'))"]},{"cell_type":"code","execution_count":null,"id":"1fe90755-9203-45e1-a47b-51bedff337e1","metadata":{"id":"1fe90755-9203-45e1-a47b-51bedff337e1"},"outputs":[],"source":["# We can confirm the shape of each our dense embeddings is 384:\n","\n","# Make binary lists to keep track of any shapes that are *not* 384\n","freshdisk_assertion = [0 for i in freshdisk_dembeddings if i.shape == 384]\n","hnsw_assertion = [0 for i in hnsw_dembeddings if i.shape == 384]\n","ivfpq_assertion = [0 for i in ivfpq_dembeddings if i.shape == 384]\n","\n","# Sum up our lists. If there are any embeddings that are not of shape 384, these sums will be > 0\n","assert sum(freshdisk_assertion) == 0\n","assert sum(hnsw_assertion) == 0\n","assert sum(ivfpq_assertion) == 0"]},{"cell_type":"markdown","id":"cffb7427-6db0-42c4-ba3e-b589edadd3e2","metadata":{"id":"cffb7427-6db0-42c4-ba3e-b589edadd3e2"},"source":["# Create Sparse Embeddings of our Chunks\n","\n","Now we can create our sparse embeddings. We will use the BM25 algorithm to create our sparse embeddings. The resulting vector will represent an inverted index of the tokens in our chunks, constrained by things like chunk length.\n","\n","Pinecone has an awesome [text library](https://github.com/pinecone-io/pinecone-text) that makes generating these vectors super easy. We also have [a great notebook](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/search/semantic-search/sparse/bm25/bm25-vector-generation.ipynb) all about BM25 encodings.\n","\n","Since we're using a ML-implemented version of BM25, we need to \"fit\" the model to our corpus. To do this, we'll combine all 3 of our PDFs together, so that the BM25 model can compute all the token frequencies etc correctly. We'll then encode each of our documents with our \"fitted\" model."]},{"cell_type":"code","execution_count":null,"id":"2aa80a1a-bea3-4d49-90cc-33206e604899","metadata":{"id":"2aa80a1a-bea3-4d49-90cc-33206e604899"},"outputs":[],"source":["# Join the content of all our PDFs together into 1 large corpus\n","\n","corpus = \"\"\n","\n","for i, v in chunked_files.items():\n","    corpus += ' '.join(v)"]},{"cell_type":"code","execution_count":null,"id":"fa6c477c-df4a-4b78-b0a6-421527650000","metadata":{"id":"fa6c477c-df4a-4b78-b0a6-421527650000"},"outputs":[],"source":["len(corpus)  # Awesome, we've got lots o' tokens here for our BM25 model to learn :)"]},{"cell_type":"code","execution_count":null,"id":"f124295c-9da5-4195-a57f-1a53f2feb842","metadata":{"id":"f124295c-9da5-4195-a57f-1a53f2feb842"},"outputs":[],"source":["# Initialize BM25 and fit to our corpus\n","\n","bm25 = BM25Encoder()\n","bm25.fit(corpus)  # takes ~30s"]},{"cell_type":"code","execution_count":null,"id":"73b2ca25-2ac4-4712-bd8d-77607ef9c852","metadata":{"id":"73b2ca25-2ac4-4712-bd8d-77607ef9c852"},"outputs":[],"source":["# Create embeddings for each chunk\n","freshdisk_sembeddings = [bm25.encode_documents(i) for i in chunked_files.get('freshdisk')]"]},{"cell_type":"code","execution_count":null,"id":"e4b1c9f3-9192-4e46-afa2-6b4e93ebb6a0","metadata":{"id":"e4b1c9f3-9192-4e46-afa2-6b4e93ebb6a0"},"outputs":[],"source":["hnsw_sembeddings = [bm25.encode_documents(i) for i in chunked_files.get('hnsw')]"]},{"cell_type":"code","execution_count":null,"id":"87899d0e-2bcd-4fc5-8c47-f27672b8dcd9","metadata":{"id":"87899d0e-2bcd-4fc5-8c47-f27672b8dcd9"},"outputs":[],"source":["ivfpq_sembeddings = [bm25.encode_documents(i) for i in chunked_files.get('ivfpq')]"]},{"cell_type":"markdown","id":"5f43b724-acdc-4c13-b321-5c63b4a3b907","metadata":{"id":"5f43b724-acdc-4c13-b321-5c63b4a3b907"},"source":["Let's look at the sparse embeddings for one of our PDFs.\n","\n","You'll see that each PDF's chunks has now transformed into a dictionary with `indices` and `values` keys."]},{"cell_type":"code","execution_count":null,"id":"a794f792-b1b5-458b-8f37-fd9e132cf907","metadata":{"id":"a794f792-b1b5-458b-8f37-fd9e132cf907"},"outputs":[],"source":["freshdisk_sembeddings"]},{"cell_type":"code","execution_count":null,"id":"02f495a3-37b8-4573-afca-3c8bfe6fb08f","metadata":{"id":"02f495a3-37b8-4573-afca-3c8bfe6fb08f"},"outputs":[],"source":["# We want the # of chunks per PDF to be equal to the # of sparse embeddings we've generated. Let's check that:\n","\n","assert len(freshdisk_sembeddings) == len(chunked_files.get('freshdisk'))\n","assert len(hnsw_sembeddings) == len(chunked_files.get('hnsw'))\n","assert len(ivfpq_sembeddings) == len(chunked_files.get('ivfpq'))"]},{"cell_type":"markdown","id":"ece747e9-abc4-4289-aac2-b2fef0831339","metadata":{"id":"ece747e9-abc4-4289-aac2-b2fef0831339"},"source":["# Getting Our Embeddings into Pinecone\n","\n","Now that we have made our sparse and dense embeddings, it's time to index them into our Pinecone index.\n","\n","One thing to note is that only [p1 and s1 pods support hybrid search](https://docs.pinecone.io/docs/indexes). Since we're not concerned about high throughput for a demo, we'll go with s1, which is optimized for storage over throughput.\n","\n","Hybrid search indexes inherently also need `\"dotproduct\"` as their similarity `metric`."]},{"cell_type":"code","source":["from pinecone import Pinecone, ServerlessSpec\n","\n","# Initialize Pinecone\n","pc = Pinecone(api_key=pinecone_api_key)\n","\n","# Choose a name for our index\n","index_name = \"hybrid-search-demo\"\n","\n","# Create our index\n","pc.create_index(\n","    name=index_name,\n","    dimension=384,  # must match the dimensionality of our (dense) vectors\n","    metric=\"dotproduct\",\n","    spec=ServerlessSpec(\n","        cloud=\"aws\",\n","        region=\"us-east-1\"  # choose an appropriate region\n","    )\n",")"],"metadata":{"id":"1Dkn9Dh-cBdW"},"id":"1Dkn9Dh-cBdW","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"9bb3e59a-2656-457f-aa88-e54fa60b2a63","metadata":{"id":"9bb3e59a-2656-457f-aa88-e54fa60b2a63"},"source":["We'll create an index object out of the index we just made."]},{"cell_type":"code","execution_count":null,"id":"5f20b672-860d-4f9a-aa9a-6aff955542d8","metadata":{"id":"5f20b672-860d-4f9a-aa9a-6aff955542d8"},"outputs":[],"source":["index = pc.Index(\"hybrid-search-demo\")"]},{"cell_type":"markdown","id":"89dc6979-bd52-4080-b7cc-80689b6daf91","metadata":{"id":"89dc6979-bd52-4080-b7cc-80689b6daf91"},"source":["We'll need to make unique IDs for all of our objects, which is easy with the `uuid` library in Python:"]},{"cell_type":"code","execution_count":null,"id":"06205558-1994-466b-b1fd-f8b5a824e742","metadata":{"id":"06205558-1994-466b-b1fd-f8b5a824e742"},"outputs":[],"source":["def create_ids(chunks: str) -> List[str]:\n","    \"\"\"\n","    Create unique IDs for each document (chunk) in our index.\n","\n","    :param chunks: Chunks of our PDF file.\n","\n","    :return: Unique IDs for chunks.\n","    \"\"\"\n","    return [str(uuid4()) for _ in range(len(chunks))]"]},{"cell_type":"code","execution_count":null,"id":"468968b1-dfc1-495c-8de1-dd8c40cf43e2","metadata":{"id":"468968b1-dfc1-495c-8de1-dd8c40cf43e2"},"outputs":[],"source":["freshdisk_ids = create_ids(chunked_files.get('freshdisk'))\n","hnsw_ids = create_ids(chunked_files.get('hnsw'))\n","ivfpq_ids = create_ids(chunked_files.get('ivfpq'))\n"]},{"cell_type":"code","execution_count":null,"id":"a855b9ff-7dcc-4543-933f-64815f9d8972","metadata":{"id":"a855b9ff-7dcc-4543-933f-64815f9d8972"},"outputs":[],"source":["# Let's preview one of our IDs:\n","\n","freshdisk_ids[0]"]},{"cell_type":"code","execution_count":null,"id":"46051402-3950-4c6c-a0ab-ac6b3a08e389","metadata":{"id":"46051402-3950-4c6c-a0ab-ac6b3a08e389"},"outputs":[],"source":["# Let's make sure we have the same # of IDs as there are chunks:\n","\n","assert len(freshdisk_ids) == len(chunked_files.get('freshdisk'))\n","assert len(hnsw_ids) == len(chunked_files.get('hnsw'))\n","assert len(ivfpq_ids) == len(chunked_files.get('ivfpq'))"]},{"cell_type":"markdown","id":"7eef3d9c-94f0-40d6-8c6c-7fef149c4152","metadata":{"id":"7eef3d9c-94f0-40d6-8c6c-7fef149c4152"},"source":["Now that we have our IDs, we can make our composite sparse-dense objects that we'll index into Pinecone. These will take 4 components:\n","- Our IDs\n","- Our sparse embeddings\n","- Our dense embeddings\n","- Our chunks\n","\n","We'll use the actual text content of our PDFs (stored in our chunks) as metadata. This allows the end user to see the content of what's being returned by their search instead of just the sparse/dense vectors. In order to store our chunks' textual data in digestible metadata object for Pinecone, we'll want to turn each chunk into a dict that has a `'text'` key to hold the chunk value."]},{"cell_type":"code","execution_count":null,"id":"9f994d0a-6167-4157-8e7f-0e450d164197","metadata":{"id":"9f994d0a-6167-4157-8e7f-0e450d164197"},"outputs":[],"source":["def create_metadata_objs(doc: List[str]) -> List[dict[str]]:\n","    \"\"\"\n","    Create objects to store as metadata alongside our sparse and dense vectors in our hybird Pinecone index.\n","\n","    :param doc: Chunks of a document we'd like to use while creating metadata objects.\n","\n","    :return: Metadata objects with a \"text\" key and a value that points to the text content of each chunk.\n","    \"\"\"\n","    return [{'text': d} for d in doc]"]},{"cell_type":"code","execution_count":null,"id":"874f6265-5c21-4169-a8b9-2bd65f5b4a57","metadata":{"id":"874f6265-5c21-4169-a8b9-2bd65f5b4a57"},"outputs":[],"source":["freshdisk_metadata = create_metadata_objs(chunked_files.get('freshdisk'))\n","hnsw_metadata = create_metadata_objs(chunked_files.get('hnsw'))\n","ivfpq_metadata = create_metadata_objs(chunked_files.get('ivfpq'))"]},{"cell_type":"code","execution_count":null,"id":"f6a6a557-49cc-4dbf-9c00-f02c447c4c48","metadata":{"id":"f6a6a557-49cc-4dbf-9c00-f02c447c4c48"},"outputs":[],"source":["# Preview\n","\n","freshdisk_metadata[0]"]},{"cell_type":"code","execution_count":null,"id":"9e2add63-efaa-4022-bf18-1472ae8aacee","metadata":{"id":"9e2add63-efaa-4022-bf18-1472ae8aacee"},"outputs":[],"source":["def create_composite_objs(ids: str, sembeddings: List[Dict[str, List[Any]]], dembeddings: List[float], metadata: Dict[str, str]) -> List[Dict[str, Any]]:\n","    \"\"\"\n","    Create objects for indexing into Pinecone. Each object contains a document ID (which corresponds to the chunk, not the larger document),\n","    the chunk's sparse embedding, the chunk's dense embedding, and the chunk's corresponding metadata object.\n","\n","    :param ids: Unique ID of a chunk we want to index.\n","    :param sembeddings: Sparse embedding representation of a chunk we want to index.\n","    :param dembeddings: Dense embedding representation of a chunk we want to index.\n","    :param metadata: Metadata objects with a \"text\" key and a value that points to the text content of each chunk.\n","\n","    :return: Composite objects in the correct format for ingest into Pinecone.\n","    \"\"\"\n","    to_index = []\n","\n","    for i in range(len(metadata)):\n","        to_index_obj = {\n","                'id': ids[i],\n","                'sparse_values': sembeddings[i],\n","                'values': dembeddings[i],\n","                'metadata': metadata[i]\n","            }\n","        to_index.append(to_index_obj)\n","    return to_index"]},{"cell_type":"code","execution_count":null,"id":"57f44a46-a6b8-4778-bab7-429e8733c181","metadata":{"id":"57f44a46-a6b8-4778-bab7-429e8733c181"},"outputs":[],"source":["freshdisk_com_objs = create_composite_objs(freshdisk_ids, freshdisk_sembeddings, freshdisk_dembeddings, freshdisk_metadata)\n","hnsw_com_objs = create_composite_objs(hnsw_ids, hnsw_sembeddings, hnsw_dembeddings, hnsw_metadata)\n","ivfpq_com_objs = create_composite_objs(ivfpq_ids, ivfpq_sembeddings, ivfpq_dembeddings, ivfpq_metadata)"]},{"cell_type":"code","execution_count":null,"id":"938a4954-ecf5-4491-87e6-597a83956024","metadata":{"id":"938a4954-ecf5-4491-87e6-597a83956024"},"outputs":[],"source":["freshdisk_com_objs[0]"]},{"cell_type":"markdown","id":"db7bc28e-5ead-4d44-a2af-287ee3efd712","metadata":{"id":"db7bc28e-5ead-4d44-a2af-287ee3efd712"},"source":["Now we can index (\"upsert\") our objects into our Pinecone index!"]},{"cell_type":"code","execution_count":null,"id":"6aaaaf91-2353-430b-aec6-0e843c499bd7","metadata":{"id":"6aaaaf91-2353-430b-aec6-0e843c499bd7"},"outputs":[],"source":["index.upsert(freshdisk_com_objs)\n","index.upsert(hnsw_com_objs)\n","index.upsert(ivfpq_com_objs)\n"]},{"cell_type":"code","execution_count":null,"id":"29feb6c2-e6ad-47c4-834a-91ba08a72927","metadata":{"id":"29feb6c2-e6ad-47c4-834a-91ba08a72927"},"outputs":[],"source":["\n","index.describe_index_stats()"]},{"cell_type":"markdown","id":"4f16c883-9892-49a6-99b5-846b356b3258","metadata":{"id":"4f16c883-9892-49a6-99b5-846b356b3258"},"source":["# Query Our Hybrid Docs\n","\n","Now that we have all of our hybrid vector objects in our Pinecone index, we can issue some queries!\n","\n","Since issuing a query to a vector index requires the query to be vectorized in the same way as the objects in the index are vectorized (so they can match up in vector space), for hybrid queries we'll have to vectorize the query *twice*! Once as a sparse vector and once as a dense vector. We then send both of those vectors to Pinecone to get items back."]},{"cell_type":"code","execution_count":null,"id":"3802bd10-1a66-4a9b-b5c6-56a2779f30d9","metadata":{"id":"3802bd10-1a66-4a9b-b5c6-56a2779f30d9"},"outputs":[],"source":["query = \"What are nearest neighbors?\"\n"]},{"cell_type":"markdown","id":"c0303a36-1bb0-40d5-85a3-eed31d003cab","metadata":{"id":"c0303a36-1bb0-40d5-85a3-eed31d003cab","jp-MarkdownHeadingCollapsed":true},"source":["Create sparse embedding from query\n","\n","Note: do *not* refit the bm25 model here. We want to keep the token frequencies etc from when we fit it to the text from our PDFs!\n","\n","You might be wondering how the model gets \"refit\" when the corpus changes, the answer is a little complicated, but essentially this is a special implementation of BM25 (which usually runs online) that has precomputed frequencies for English words, based off the MSMarco dataset. So, when you add new docs to the corpus, you don't have to \"refit\" the BM25 model, it just finds the word frequencies in the MSMarco dataset.\n","\n","More here: https://github.com/pinecone-io/pinecone-text/blob/main/pinecone_text/sparse/bm25_encoder.py#L255\n","\n"]},{"cell_type":"code","execution_count":null,"id":"fd44e1c5-5ccc-4e94-8247-d83f2a32f02a","metadata":{"id":"fd44e1c5-5ccc-4e94-8247-d83f2a32f02a"},"outputs":[],"source":["query_sembedding = bm25.encode_queries(query)"]},{"cell_type":"code","execution_count":null,"id":"4644d3a3-d5e3-4e6e-8383-ccd214bd0eaf","metadata":{"id":"4644d3a3-d5e3-4e6e-8383-ccd214bd0eaf"},"outputs":[],"source":["# Cool! We can see there are only two values in here, because BM25 automatically removed stop word like \"what\" and \"is\"\n","\n","query_sembedding"]},{"cell_type":"code","execution_count":null,"id":"20e4ad1b-418b-4865-9995-45bb8a351efb","metadata":{"id":"20e4ad1b-418b-4865-9995-45bb8a351efb","scrolled":true},"outputs":[],"source":["# Create dense embedding\n","\n","query_dembedding = produce_embeddings([query])"]},{"cell_type":"code","execution_count":null,"id":"c366ef74-e1cd-46d1-b7a2-e856acb6ce8a","metadata":{"id":"c366ef74-e1cd-46d1-b7a2-e856acb6ce8a","scrolled":true},"outputs":[],"source":["query_dembedding"]},{"cell_type":"markdown","id":"06ebc12c-a6a2-4834-b6ec-749fd61f32ae","metadata":{"id":"06ebc12c-a6a2-4834-b6ec-749fd61f32ae"},"source":["Pinecone vector search has a cool user feature where you can weight the sparse vectors higher or lower (i.e. of more or less importance) than the dense vectors. This is controlled by the `alpha` parameter. An `alpha` of 0 means you're doing a totally keyword-based search (i.e. only over sparse vectors), while an `alpha` of 1 means you're doing a totally semantic search (i.e. only over dense vectors).\n","\n","Let's make a function that'll let us weight our vectors by alpha.\n","\n","(We'll also include `k`, which is the number of docs we want to retrieve)"]},{"cell_type":"code","execution_count":null,"id":"6b61142a-192e-4b73-87f1-eeb68e8c4998","metadata":{"id":"6b61142a-192e-4b73-87f1-eeb68e8c4998"},"outputs":[],"source":["# Integrate alpha and top-k\n","\n","def weight_by_alpha(sparse_embedding: Dict[str, List[Any]], dense_embedding: List[float], alpha: float) -> Tuple[Dict[str, List[Any]], List[float]]:\n","    \"\"\"\n","    Weight the values of our sparse and dense embeddings by the parameter alpha (0-1).\n","\n","    :param sparse_embedding: Sparse embedding representation of one of our documents (or chunks).\n","    :param dense_embedding: Dense embedding representation of one of our documents (or chunks).\n","    :param alpha: Weighting parameter between 0-1 that controls the impact of sparse or dense embeddings on the retrieval and ranking\n","        of returned docs (chunks) in our index.\n","\n","    :return: Weighted sparse and dense embeddings for one of our documents (chunks).\n","    \"\"\"\n","    if alpha < 0 or alpha > 1:\n","        raise ValueError(\"Alpha must be between 0 and 1\")\n","    hsparse = {\n","        'indices': sparse_embedding['indices'],\n","        'values':  [v * (1 - alpha) for v in sparse_embedding['values']]\n","    }\n","    hdense = [v * alpha for v in dense_embedding]\n","    return hsparse, hdense"]},{"cell_type":"markdown","id":"f0c12e86-5a18-41cf-bce0-699df56fbc8c","metadata":{"id":"f0c12e86-5a18-41cf-bce0-699df56fbc8c"},"source":["Now let's make a function that'll query our Pinecone index while taking into account whatever `alpha` and `k` values we want to pass:"]},{"cell_type":"code","execution_count":null,"id":"787c9898-b1d8-4eea-a8ec-5b24215c2a27","metadata":{"id":"787c9898-b1d8-4eea-a8ec-5b24215c2a27"},"outputs":[],"source":["# Note this doesn't have any genAI in it yet\n","\n","\n","def issue_hybrid_query(sparse_embedding: Dict[str, List[Any]], dense_embedding: List[float], alpha: float, top_k: int) -> QueryResponse:\n","    \"\"\"\n","    Send properly formatted hybrid search query to Pinecone index and get back `k` ranked results (ranked by dot product similarity, as\n","        defined when we made our index).\n","\n","    :param sparse_embedding: Sparse embedding representation of one of our documents (or chunks).\n","    :param dense_embedding: Dense embedding representation of one of our documents (or chunks).\n","    :param alpha: Weighting parameter between 0-1 that controls the impact of sparse or dense embeddings on the retrieval and ranking\n","        of returned docs (chunks) in our index.\n","    :param top_k: The number of documents (chunks) we want back from Pinecone.\n","\n","    :return: QueryResponse object from Pinecone containing top-k results.\n","    \"\"\"\n","    scaled_sparse, scaled_dense = weight_by_alpha(sparse_embedding, dense_embedding, alpha)\n","\n","    result = index.query(\n","        vector=scaled_dense,\n","        sparse_vector=scaled_sparse,\n","        top_k=top_k,\n","        include_metadata=True\n","    )\n","    return result"]},{"cell_type":"markdown","id":"626bffe8-cce1-47b9-84d6-c827e9f5cf07","metadata":{"id":"626bffe8-cce1-47b9-84d6-c827e9f5cf07"},"source":["Let's issue a pure semantic search:"]},{"cell_type":"code","execution_count":null,"id":"cea14e5f-20dc-44a1-8793-17583e8689ef","metadata":{"id":"cea14e5f-20dc-44a1-8793-17583e8689ef","scrolled":true},"outputs":[],"source":["# Note, for our dense embedding (`query_dembedding`), we need to grab the 1st value [0] since Pinecone expects a Numpy array when queried:\n","\n","issue_hybrid_query(query_sembedding, query_dembedding[0], 1.0, 5)"]},{"cell_type":"markdown","id":"d98e72fd-4ace-42dc-8bd2-d086bd0907f4","metadata":{"id":"d98e72fd-4ace-42dc-8bd2-d086bd0907f4","scrolled":true},"source":["And now a pure keyword search. You can see how many more domain-specific words are in these results:"]},{"cell_type":"code","execution_count":null,"id":"f6146883-735d-4755-84fc-f234a92725be","metadata":{"id":"f6146883-735d-4755-84fc-f234a92725be","scrolled":true},"outputs":[],"source":["issue_hybrid_query(query_sembedding, query_dembedding[0], 0.0, 5)"]},{"cell_type":"markdown","id":"623df78c-e1db-440a-97fb-25d97e6678f0","metadata":{"id":"623df78c-e1db-440a-97fb-25d97e6678f0"},"source":["You can see the differences above: when we issue a purely semantic search, our search results are about what the idea of \"nearest neighbors\" is; in our keyword search, the vast majority of our search results are just exact-word matches for the tokens \"nearest\" and \"neighbors\". Most of them are just citations from the HNSW article's bibliography!\n","\n","Can we get the best of both worlds? In an ideal world, my search results would both tell me \"about\" the concept of nearest neighbors and contain things like citations that I could read more about later.\n","\n","Let's see if we can get a combination of semantic and keyword search by toggling our `alpha` value:"]},{"cell_type":"code","execution_count":null,"id":"a19923ef-af08-41cb-976b-f93fd4e0148d","metadata":{"id":"a19923ef-af08-41cb-976b-f93fd4e0148d","scrolled":true},"outputs":[],"source":["issue_hybrid_query(query_sembedding, query_dembedding[0], 0.2, 5)  # closer to 1.0 = closer to pure keyword search"]},{"cell_type":"markdown","id":"6f0e73e1-4ae3-4237-91bc-cea84a810a72","metadata":{"id":"6f0e73e1-4ae3-4237-91bc-cea84a810a72"},"source":["Amazing! You can see that our first couple search results are not very different than our pure keyword search. But when you get further down the results list, you'll see that we get an equation we can use to calculate KNN. That's a bit more useful than #3 in our pure keyword search, which is a bibliography entry. That's likely because we have semantic search in there too -- Pinecone knows we want to know \"about\" KNN, so it fetches items with lots of domain-specific terms (keyword search), but also items that demonstrate the \"aboutness\" of KNN (semantic search).\n"]},{"cell_type":"markdown","id":"95fec382-1f9d-4e23-abbe-9897a00d531c","metadata":{"id":"95fec382-1f9d-4e23-abbe-9897a00d531c"},"source":["# Let's take a closer look. For science!"]},{"cell_type":"code","execution_count":null,"id":"8b16ea1f-e70a-4c44-af0a-95666cd212d8","metadata":{"id":"8b16ea1f-e70a-4c44-af0a-95666cd212d8"},"outputs":[],"source":["pure_keyword = issue_hybrid_query(query_sembedding, query_dembedding[0], 1.0, 5)\n","pure_semantic = issue_hybrid_query(query_sembedding, query_dembedding[0], 0.0, 5)\n","hybrid_1 = issue_hybrid_query(query_sembedding, query_dembedding[0], 0.1, 5)\n","hybrid_2 = issue_hybrid_query(query_sembedding, query_dembedding[0], 0.2, 5)\n","hybrid_3 = issue_hybrid_query(query_sembedding, query_dembedding[0], 0.3, 5)\n","hybrid_4 = issue_hybrid_query(query_sembedding, query_dembedding[0], 0.4, 5)\n","hybrid_5 = issue_hybrid_query(query_sembedding, query_dembedding[0], 0.5, 5)"]},{"cell_type":"code","execution_count":null,"id":"6de66f2e-7a86-4711-837b-8dfd19785e27","metadata":{"id":"6de66f2e-7a86-4711-837b-8dfd19785e27","scrolled":true},"outputs":[],"source":["# Let's turn these all into dataframes and see the different rankings\n","# Feel free to skip this part (it's just an interesting side journey)\n","\n","import pandas as pd\n","\n","df = pd.concat([\n","    pd.DataFrame([(i['metadata']['text'], i['score'], 'keyword') for i in pure_keyword.get('matches')]),\n","    pd.DataFrame([(i['metadata']['text'], i['score'], 'semantic') for i in pure_semantic.get('matches')]),\n","    pd.DataFrame([(i['metadata']['text'], i['score'], 'hybrid_1') for i in hybrid_1.get('matches')]),\n","    pd.DataFrame([(i['metadata']['text'], i['score'], 'hybrid_2') for i in hybrid_2.get('matches')]),\n","    pd.DataFrame([(i['metadata']['text'], i['score'], 'hybrid_3') for i in hybrid_3.get('matches')]),\n","    pd.DataFrame([(i['metadata']['text'], i['score'], 'hybrid_4') for i in hybrid_4.get('matches')]),\n","    pd.DataFrame([(i['metadata']['text'], i['score'], 'hybrid_5') for i in hybrid_5.get('matches')]),\n","]).rename(columns={0: 'document', 1: 'score', 2: 'search_type'})\n","\n","# Note: don't pay too much attention to the \"score\" column. This really only matters within the same type of search, for ranking docs.\n","# Don't use it to compare *across* different search types (e.g. keyword search isn't inherently more relevant simply because it has higher\n","# scores overall)"]},{"cell_type":"code","execution_count":null,"id":"9912ce6a-e3bf-48f1-9e51-65c8cc3d8691","metadata":{"id":"9912ce6a-e3bf-48f1-9e51-65c8cc3d8691"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"id":"eb6cb84e-21b9-4cd0-b1f1-06c077ea58c0","metadata":{"id":"eb6cb84e-21b9-4cd0-b1f1-06c077ea58c0","scrolled":true},"outputs":[],"source":["# Let's give each document a label so that it's easier to see their ranking differences per search type\n","\n","from sklearn import preprocessing\n","label_encoder = preprocessing.LabelEncoder()\n","df['document_encoded'] = label_encoder.fit_transform(df['document'])\n","\n","df.head().sort_values(['document_encoded'])"]},{"cell_type":"code","execution_count":null,"id":"f6945cec-025c-4425-831e-0aaeb4a83cf6","metadata":{"id":"f6945cec-025c-4425-831e-0aaeb4a83cf6","scrolled":true},"outputs":[],"source":["for i, v in df.groupby(['search_type']):\n","    print(v[['search_type', 'document_encoded', 'score']])\n"]},{"cell_type":"markdown","id":"b268e1e0-d59b-4d40-a19d-f084a8b108b9","metadata":{"id":"b268e1e0-d59b-4d40-a19d-f084a8b108b9"},"source":["Above, you can see the subtle ranking differences across each search type. For the most part, `document 8` is the top documents, except in `hybrid_1`, `hybrid_2` and `semantic`. In those two search types, `document 10` is the top document.\n","\n","It's up to you and your stakeholders to find the ideal `alpha` for your use case(s).\n","\n","Directly, for our use case, it seems anything >= `alpha=0.3` gets us similar results, so the impact of `alpha` is most discernable between `0.0-0.3`.\n","\n","Cool!"]},{"cell_type":"markdown","id":"5bb6ece2-cb19-43c0-9ab5-e0c0c09b46a2","metadata":{"editable":true,"id":"5bb6ece2-cb19-43c0-9ab5-e0c0c09b46a2","tags":[]},"source":["# Incorporating GenAI\n","\n","Now, hybrid search is cool enough, but what if you don't want to spend time sifting through your index's search results? What if you just want a single answer to a query?\n","\n","That's where GenAI comes in.\n","\n","We will make a retrieval augmented generation (RAG) pipeline that will make this happen.\n","\n","Since large language models (LLMs) do not know a ton of specific information (they are trained on the general Internet), especially if the information is from PDFs that it would have to download to have access to (like what are in our index), we need to give it this information!\n","\n","We do this by first sending our query to our Pinecone index and grabbing some search  results. We then attach these search results to our original query and send *both* to the LLM. That way, the LLM both knows what we want to ask it & can pull from its general knowledge store *and* has a specialized knowledge store (our Pinecone search results so that it can get us extra specific information.\n","\n","Let's try it out:"]},{"cell_type":"code","execution_count":null,"id":"9cefb103-621c-4a43-b4e6-caeec5dfccc1","metadata":{"id":"9cefb103-621c-4a43-b4e6-caeec5dfccc1"},"outputs":[],"source":["# Let's grab the textual metadata from our search results:\n","\n","hybrid_context = [i.get('metadata').get('text') for i in hybrid_3.get('matches')]\n","pure_keyword_context = [i.get('metadata').get('text') for i in pure_keyword.get('matches')]\n","pure_semantic_context = [i.get('metadata').get('text') for i in pure_semantic.get('matches')]"]},{"cell_type":"code","execution_count":null,"id":"11788caa-ea17-4678-93de-871f74d3057f","metadata":{"id":"11788caa-ea17-4678-93de-871f74d3057f"},"outputs":[],"source":["# We are then going to combine this \"context\" with our original query in a format that our LLM likes:\n","\n","hybrid_augmented_query = \"\\n\\n---\\n\\n\".join(hybrid_context)+\"\\n\\n-----\\n\\n\"+query\n","pure_keyword_augmented_query = \"\\n\\n---\\n\\n\".join(pure_keyword_context)+\"\\n\\n-----\\n\\n\"+query\n","pure_semantic_augmented_query = \"\\n\\n---\\n\\n\".join(pure_keyword_context)+\"\\n\\n-----\\n\\n\"+query"]},{"cell_type":"code","execution_count":null,"id":"705cdd94-39ad-493e-b55b-12516b20ea1e","metadata":{"id":"705cdd94-39ad-493e-b55b-12516b20ea1e","scrolled":true},"outputs":[],"source":["print(hybrid_augmented_query)"]},{"cell_type":"code","execution_count":null,"id":"384afe12-83bf-4c10-baf0-0241ecd8132f","metadata":{"id":"384afe12-83bf-4c10-baf0-0241ecd8132f"},"outputs":[],"source":["# We are then going to give our LLM some instructions for how to act:\n","\n","primer = f\"\"\"You are Q&A bot. A highly intelligent system that answers\n","user questions based on the information provided by the user above\n","each question. If the information can not be found in the information\n","provided by the user you truthfully say \"I don't know\".\n","\"\"\""]},{"cell_type":"code","execution_count":null,"id":"cf840d48-be91-4dd3-b7bb-9d0556da0b80","metadata":{"id":"cf840d48-be91-4dd3-b7bb-9d0556da0b80","scrolled":true},"outputs":[],"source":["# Now we query our LLM with our augmented query & our primer!\n","\n","# Our hybrid query:\n","\n","openai.api_key = openai_api_key\n","\n","\n","hybrid_res = openai.ChatCompletion.create(\n","    model=\"gpt-4\",\n","    messages=[\n","        {\"role\": \"system\", \"content\": primer},\n","        {\"role\": \"user\", \"content\": hybrid_augmented_query}\n","    ]\n",")\n","\n","hybrid_res"]},{"cell_type":"code","execution_count":null,"id":"43a229cf-3b8c-4d75-b863-f6091c108811","metadata":{"id":"43a229cf-3b8c-4d75-b863-f6091c108811","scrolled":true},"outputs":[],"source":["# Our pure_keyword query:\n","\n","pure_keyword_res = openai.ChatCompletion.create(\n","    model=\"gpt-4\",\n","    messages=[\n","        {\"role\": \"system\", \"content\": primer},\n","        {\"role\": \"user\", \"content\": pure_keyword_augmented_query}\n","    ]\n",")\n","\n","pure_keyword_res"]},{"cell_type":"code","execution_count":null,"id":"c198628f-ff63-414e-bd90-eb2fdb07df47","metadata":{"id":"c198628f-ff63-414e-bd90-eb2fdb07df47","scrolled":true},"outputs":[],"source":["# Our pure_semantic query:\n","\n","pure_semantic_res = openai.ChatCompletion.create(\n","    model=\"gpt-4\",\n","    messages=[\n","        {\"role\": \"system\", \"content\": primer},\n","        {\"role\": \"user\", \"content\": pure_semantic_augmented_query}\n","    ]\n",")\n","\n","pure_semantic_res"]},{"cell_type":"markdown","id":"466f61d4-1896-4716-a8e4-b25a4eda97fe","metadata":{"id":"466f61d4-1896-4716-a8e4-b25a4eda97fe"},"source":["You can see subtle differences across the different results above. It's up to you and your stakeholders to figure out what type of search (semantic, keyword, hybrid) offers the most relevant information for your end users"]},{"cell_type":"markdown","id":"d28051b6-219a-44bf-aee7-5894d6af600b","metadata":{"id":"d28051b6-219a-44bf-aee7-5894d6af600b"},"source":["# What if we take our our Pinecone vectors altogether??"]},{"cell_type":"code","execution_count":null,"id":"6837f020-3147-4014-b31b-f0b070ae9a69","metadata":{"id":"6837f020-3147-4014-b31b-f0b070ae9a69","scrolled":true},"outputs":[],"source":["# What if we issue our original query without our Pinecone vectors as context?\n","\n","res = openai.ChatCompletion.create(\n","    model=\"gpt-4\",\n","    messages=[\n","        {\"role\": \"system\", \"content\": primer},\n","        {\"role\": \"user\", \"content\": query}\n","    ]\n",")\n","\n","res"]},{"cell_type":"markdown","id":"2b6943af-c430-4a5c-acd7-2a55f53a8f87","metadata":{"id":"2b6943af-c430-4a5c-acd7-2a55f53a8f87"},"source":["We can see that RAG really does have a huge impact! Without our PDFs, ChatGPT doesn't know much helpful detail at all! Nor can it give us bibliographic data for articles we might want to look up later!"]},{"cell_type":"markdown","id":"fc32f342-33ad-47cf-a11b-4b7981405b76","metadata":{"id":"fc32f342-33ad-47cf-a11b-4b7981405b76","scrolled":true},"source":["# All finished!\n","\n","Check out [the documentation on hybrid search](https://docs.pinecone.io/docs/hybrid-search-and-sparse-vectors) and keep building awesome things!"]},{"cell_type":"code","source":[],"metadata":{"id":"vEIl3lbeejaw"},"id":"vEIl3lbeejaw","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1ILYFd7k_SIAJry3DUy_XQGrSe7OhHGeG","timestamp":1728914247958},{"file_id":"https://github.com/pinecone-io/examples/blob/master/learn/generation/rag-for-hybrid/rag-for-hybrid-search.ipynb","timestamp":1727000354429}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}